{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a57b6245-a6ed-4d62-8e07-67be85a3a2e1",
   "metadata": {},
   "source": [
    "# Tutorial: Model order reduction with artificial neural networks\n",
    "\n",
    "Recent success of artificial neural networks led to the development of several\n",
    "methods for model order reduction using neural networks. pyMOR provides the\n",
    "functionality for a simple approach developed by Hesthaven and Ubbiali in {cite}`HU18`.\n",
    "For training and evaluation of the neural networks, [PyTorch](<https://pytorch.org>) is used.\n",
    "\n",
    "In this tutorial we will learn about feedforward neural networks, the basic\n",
    "idea of the approach by Hesthaven et al., and how to use it in pyMOR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17638ab-1673-4eca-92e4-d82d67ba4f2c",
   "metadata": {},
   "source": [
    "## Feedforward neural networks\n",
    "\n",
    "We aim at approximating a mapping $h\\colon\\mathcal{P}\\rightarrow Y$\n",
    "between some input space $\\mathcal{P}\\subset\\mathbb{R}^p$ (in our case the\n",
    "parameter space) and an output space $Y\\subset\\mathbb{R}^m$ (in our case the\n",
    "reduced space), given a set $S=\\{(\\mu_i,h(\\mu_i))\\in\\mathcal{P}\\times Y: i=1,\\dots,N\\}$\n",
    "of samples, by means of an artificial neural network. In this context, neural\n",
    "networks serve as a special class of functions that are able to \"learn\" the\n",
    "underlying structure of the sample set $S$ by adjusting their weights.\n",
    "More precisely, feedforward neural networks consist of several layers, each\n",
    "comprising a set of neurons that are connected to neurons in adjacent layers.\n",
    "A so-called \"weight\" is assigned to each of those connections. The weights in\n",
    "the neural network can be adjusted while fitting the neural network to the\n",
    "given sample set. For a given input $\\mu\\in\\mathcal{P}$, the weights between the\n",
    "input layer and the first hidden layer (the one after the input layer) are\n",
    "multiplied with the respective values in $\\mu$ and summed up. Subsequently,\n",
    "a so-called \"bias\" (also adjustable during training) is added and the result is\n",
    "assigned to the corresponding neuron in the first hidden layer. Before passing\n",
    "those values to the following layer, a (non-linear) activation function\n",
    "$\\rho\\colon\\mathbb{R}\\rightarrow\\mathbb{R}$ is applied. If $\\rho$\n",
    "is linear, the function implemented by the neural network is affine, since\n",
    "solely affine operations were performed. Hence, one usually chooses a\n",
    "non-linear activation function to introduce non-linearity in the neural network\n",
    "and thus increase its approximation capability. In some sense, the input\n",
    "$\\mu$ is passed through the neural network, affine-linearly combined with the\n",
    "other inputs and non-linearly transformed. These steps are repeated in several\n",
    "layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5bc37c-df56-4b79-ba16-d4408a294259",
   "metadata": {},
   "source": [
    "The following figure shows a simple example of a neural network with two hidden\n",
    "layers, an input size of two and an output size of three. Each edge between\n",
    "neurons has a corresponding weight that is learnable in the training phase.\n",
    "\n",
    "<center><img src=\"neural_network.svg\" alt=\"Feedforward neural network\" /></center>\n",
    "\n",
    "To train the neural network, one considers a so-called \"loss function\", that\n",
    "measures how the neural network performs on the training set {math}`S`, i.e.\n",
    "how accurately the neural network reproduces the output {math}`h(\\mu_i)` given\n",
    "the input {math}`\\mu_i`. The weights of the neural network are adjusted\n",
    "iteratively such that the loss function is successively minimized. To this end,\n",
    "one typically uses a Quasi-Newton method for small neural networks or a\n",
    "(stochastic) gradient descent method for deep neural networks (those with many\n",
    "hidden layers).\n",
    "\n",
    "A possibility to use feedforward neural networks in combination with reduced\n",
    "basis methods will be introduced in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731580a3-a90a-4fdf-a13a-9b740008bf4b",
   "metadata": {},
   "source": [
    "## A non-intrusive reduced order method using artificial neural networks\n",
    "\n",
    "We now assume that we are given a parametric pyMOR {{ Model }} for which we want\n",
    "to compute a reduced order surrogate {{ Model }} using a neural network. In this\n",
    "example, we consider the following two-dimensional diffusion problem with\n",
    "parametrized diffusion, right hand side and Dirichlet boundary condition:\n",
    "\n",
    "```{math}\n",
    "-\\nabla \\cdot \\big(\\sigma(x, \\mu) \\nabla u(x, \\mu) \\big) = f(x, \\mu),\\quad x=(x_1,x_2) \\in \\Omega,\n",
    "```\n",
    "\n",
    "on the domain {math}`\\Omega:= (0, 1)^2 \\subset \\mathbb{R}^2` with data\n",
    "functions {math}`f((x_1, x_2), \\mu) = 10 \\cdot \\mu + 0.1`,\n",
    "{math}`\\sigma((x_1, x_2), \\mu) = (1 - x_1) \\cdot \\mu + x_1`, where\n",
    "{math}`\\mu \\in (0.1, 1)` denotes the parameter. Further, we apply the\n",
    "Dirichlet boundary conditions\n",
    "\n",
    "```{math}\n",
    "u((x_1, x_2), \\mu) = 2x_1\\mu + 0.5,\\quad x=(x_1, x_2) \\in \\partial\\Omega.\n",
    "```\n",
    "\n",
    "We discretize the problem using pyMOR's builtin discretization toolkit as\n",
    "explained in {doc}`tutorial_builtin_discretizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff984d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pymor.basic import *\n",
    "\n",
    "problem = StationaryProblem(\n",
    "      domain=RectDomain(),\n",
    "\n",
    "      rhs=LincombFunction(\n",
    "          [ExpressionFunction('10', 2), ConstantFunction(1., 2)],\n",
    "          [ProjectionParameterFunctional('mu'), 0.1]),\n",
    "\n",
    "      diffusion=LincombFunction(\n",
    "          [ExpressionFunction('1 - x[0]', 2), ExpressionFunction('x[0]', 2)],\n",
    "          [ProjectionParameterFunctional('mu'), 1]),\n",
    "\n",
    "      dirichlet_data=LincombFunction(\n",
    "          [ExpressionFunction('2 * x[0]', 2), ConstantFunction(1., 2)],\n",
    "          [ProjectionParameterFunctional('mu'), 0.5]),\n",
    "\n",
    "      name='2DProblem'\n",
    "  )\n",
    "\n",
    "fom, _ = discretize_stationary_cg(problem, diameter=1/50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b7fb43",
   "metadata": {},
   "source": [
    "Since we employ a single {{ Parameter }}, and thus use the same range for each\n",
    "parameter, we can create the {{ ParameterSpace }} using the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c44cf62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameter_space = fom.parameters.space((0.1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392eec99",
   "metadata": {},
   "source": [
    "The main idea of the approach by Hesthaven et al. is to approximate the mapping\n",
    "from the {{ Parameters }} to the coefficients of the respective solution in a\n",
    "reduced basis by means of a neural network. Thus, in the online phase, one\n",
    "performs a forward pass of the {{ Parameters }} through the neural networks and\n",
    "obtains the approximated reduced coordinates. To derive the corresponding\n",
    "high-fidelity solution, one can further use the reduced basis and compute the\n",
    "linear combination defined by the reduced coefficients. The reduced basis is\n",
    "created via POD.\n",
    "\n",
    "The method described above is \"non-intrusive\", which means that no deep insight\n",
    "into the model or its implementation is required and it is completely\n",
    "sufficient to be able to generate full order snapshots for a randomly chosen\n",
    "set of parameters. This is one of the main advantages of the proposed approach,\n",
    "since one can simply train a neural network, check its performance and resort\n",
    "to a different method if the neural network does not provide proper\n",
    "approximation results.\n",
    "\n",
    "In pyMOR, there exists a training routine for feedforward neural networks. This\n",
    "procedure is part of a reductor and it is not necessary to write a custom\n",
    "training algorithm for each specific problem. However, it is sometimes\n",
    "necessary to try different architectures for the neural network to find the one\n",
    "that best fits the problem at hand. In the reductor, one can easily adjust the\n",
    "number of layers and the number of neurons in each hidden layer, for instance.\n",
    "Furthermore, it is also possible to change the deployed activation function.\n",
    "\n",
    "To train the neural network, we create a training and a validation set\n",
    "consisting of 100 and 20 randomly chosen {{ parameter_values }}, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e193f867",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_set = parameter_space.sample_uniformly(100)\n",
    "validation_set = parameter_space.sample_randomly(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f08624f",
   "metadata": {},
   "source": [
    "In this tutorial, we construct the reduced basis such that no more modes than\n",
    "required to bound the l2-approximation error by a given value are used.\n",
    "The l2-approximation error is  the error of the orthogonal projection (in the\n",
    "l2-sense) of the training snapshots onto the reduced basis. That is, we\n",
    "prescribe `l2_err` in the reductor. It is also possible to determine a relative\n",
    "or absolute tolerance (in the singular values) that should not be exceeded on\n",
    "the training set. Further, one can preset the size of the reduced basis.\n",
    "\n",
    "The training is aborted when a neural network that guarantees our prescribed\n",
    "tolerance is found. If we set `ann_mse` to `None`, this function will\n",
    "automatically train several neural networks with different initial weights and\n",
    "select the one leading to the best results on the validation set. We can also\n",
    "set `ann_mse` to `'like_basis'`. Then, the algorithm tries to train a neural\n",
    "network that leads to a mean squared error on the training set that is as small\n",
    "as the error of the reduced basis. If the maximal number of restarts is reached\n",
    "without finding a network that fulfills the tolerances, an exception is raised.\n",
    "In such a case, one could try to change the architecture of the neural network\n",
    "or switch to `ann_mse=None` which is guaranteed to produce a reduced order\n",
    "model (perhaps with insufficient approximation properties).\n",
    "\n",
    "We can now construct a reductor with prescribed error for the basis and mean\n",
    "squared error of the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce02ac5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pymor.reductors.neural_network import NeuralNetworkReductor\n",
    "\n",
    "reductor = NeuralNetworkReductor(fom,\n",
    "                                 training_set,\n",
    "                                 validation_set,\n",
    "                                 l2_err=1e-5,\n",
    "                                 ann_mse=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff482174",
   "metadata": {},
   "source": [
    "To reduce the model, i.e. compute a reduced basis via POD and train the neural\n",
    "network, we use the respective function of the\n",
    "{class}`~pymor.reductors.neural_network.NeuralNetworkReductor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47f05b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rom = reductor.reduce(restarts=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ad6c69",
   "metadata": {},
   "source": [
    "We are now ready to test our reduced model by solving for a random parameter value\n",
    "the full problem and the reduced model and visualize the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c5540b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mu = parameter_space.sample_randomly()\n",
    "\n",
    "U = fom.solve(mu)\n",
    "U_red = rom.solve(mu)\n",
    "U_red_recon = reductor.reconstruct(U_red)\n",
    "\n",
    "fom.visualize((U, U_red_recon),\n",
    "              legend=(f'Full solution for parameter {mu}', f'Reduced solution for parameter {mu}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db778fc4",
   "metadata": {},
   "source": [
    "Finally, we measure the error of our neural network and the performance\n",
    "compared to the solution of the full order problem on a training set. To this\n",
    "end, we sample randomly some {{ parameter_values }} from our {{ ParameterSpace }}:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da0f079",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = parameter_space.sample_randomly(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748a1fe6",
   "metadata": {},
   "source": [
    "Next, we create empty solution arrays for the full and reduced solutions and an\n",
    "empty list for the speedups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1cc445",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = fom.solution_space.empty(reserve=len(test_set))\n",
    "U_red = fom.solution_space.empty(reserve=len(test_set))\n",
    "\n",
    "speedups = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1a9d46",
   "metadata": {},
   "source": [
    "Now, we iterate over the test set, compute full and reduced solutions to the\n",
    "respective parameters and measure the speedup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b2221e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for mu in test_set:\n",
    "    tic = time.perf_counter()\n",
    "    U.append(fom.solve(mu))\n",
    "    time_fom = time.perf_counter() - tic\n",
    "\n",
    "    tic = time.perf_counter()\n",
    "    U_red.append(reductor.reconstruct(rom.solve(mu)))\n",
    "    time_red = time.perf_counter() - tic\n",
    "\n",
    "    speedups.append(time_fom / time_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca612a7",
   "metadata": {},
   "source": [
    "We can now derive the absolute and relative errors on the training set as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163ee0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_errors = (U - U_red).norm()\n",
    "relative_errors = (U - U_red).norm() / U.norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660ab464",
   "metadata": {},
   "source": [
    "The average absolute error amounts to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418507fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.average(absolute_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2026e452",
   "metadata": {},
   "source": [
    "On the other hand, the average relative error is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5452fea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(relative_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7492b6a",
   "metadata": {},
   "source": [
    "Using neural networks results in the following median speedup compared to\n",
    "solving the full order problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53054f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(speedups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4a0184",
   "metadata": {},
   "source": [
    "Since {class}`~pymor.reductors.neural_network.NeuralNetworkReductor` only calls\n",
    "the {meth}`~pymor.models.interface.Model.solve` method of the {{ Model }}, it can easily\n",
    "be applied to {{ Models }} originating from external solvers, without requiring any access to\n",
    "{{ Operators }} internal to the solver."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb603c8b",
   "metadata": {},
   "source": [
    "## Direct approximation of output quantities\n",
    "\n",
    "Thus far, we were mainly interested in approximating the solution state\n",
    "{math}`u(\\mu)\\equiv u(\\cdot,\\mu)` for some parameter {math}`\\mu`. If we consider an output\n",
    "functional {math}`\\mathcal{J}(\\mu):= J(u(\\mu), \\mu)`, one can use the reduced solution\n",
    "{math}`u_N(\\mu)` for computing the output as {math}`\\mathcal{J}(\\mu)\\approx J(u_N(\\mu),\\mu)`.\n",
    "However, when dealing with neural networks, one could also think about directly learning the\n",
    "mapping from parameter to output. That is, one can use a neural network to approximate\n",
    "{math}`\\mathcal{J}\\colon\\mathcal{P}\\to\\mathbb{R}^q`, where {math}`q\\in\\mathbb{N}` denotes\n",
    "the output dimension.\n",
    "\n",
    "In the following, we will extend our problem from the last section by an output functional\n",
    "and use the {class}`~pymor.reductors.neural_network.NeuralNetworkStatefreeOutputReductor` to\n",
    "derive a reduced model that can solely be used to solve for the output quantity without\n",
    "computing a reduced state at all.\n",
    "\n",
    "For the definition of the output, we define the output of out problem as the l2-product of the\n",
    "solution with the right hand side respectively Dirichlet boundary data of our original problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c2b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = problem.with_(outputs=[('l2', problem.rhs), ('l2_boundary', problem.dirichlet_data)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61838368",
   "metadata": {},
   "source": [
    "Consequently, the output dimension is {math}`q=2`. After adjusting the problem definition,\n",
    "we also have to update the full order model to be aware of the output quantities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8620fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fom, _ = discretize_stationary_cg(problem, diameter=1/50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e2f916",
   "metadata": {},
   "source": [
    "We can now import the {class}`~pymor.reductors.neural_network.NeuralNetworkStatefreeOutputReductor`\n",
    "and initialize the reductor using the same data as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7aa553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymor.reductors.neural_network import NeuralNetworkStatefreeOutputReductor\n",
    "\n",
    "output_reductor = NeuralNetworkStatefreeOutputReductor(fom,\n",
    "                                                       training_set,\n",
    "                                                       validation_set,\n",
    "                                                       validation_loss=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127bd1df",
   "metadata": {},
   "source": [
    "Similar to the `NeuralNetworkReductor`, we can call `reduce` to obtain a reduced order model.\n",
    "In this case, `reduce` trains a neural network to approximate the mapping from parameter to\n",
    "output directly. Therefore, we can only use the resulting reductor to solve for the outputs\n",
    "and not for state approximations. The `NeuralNetworkReductor` though can be used to do both by\n",
    "calling `solve` respectively `output` (if we had initialized the `NeuralNetworkReductor` with\n",
    "the problem including the output quantities).\n",
    "\n",
    "We now perform the reduction and run some tests with the resulting\n",
    "{class}`~pymor.models.neural_network.NeuralNetworkStatefreeOutputModel`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f129c440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_rom = output_reductor.reduce(restarts=100)\n",
    "\n",
    "outputs = []\n",
    "outputs_red = []\n",
    "outputs_speedups = []\n",
    "\n",
    "for mu in test_set:\n",
    "    tic = time.perf_counter()\n",
    "    outputs.append(fom.output(mu=mu))\n",
    "    time_fom = time.perf_counter() - tic\n",
    "\n",
    "    tic = time.perf_counter()\n",
    "    outputs_red.append(output_rom.output(mu=mu))\n",
    "    time_red = time.perf_counter() - tic\n",
    "\n",
    "    outputs_speedups.append(time_fom / time_red)\n",
    "\n",
    "outputs = np.squeeze(np.array(outputs))\n",
    "outputs_red = np.squeeze(np.array(outputs_red))\n",
    "\n",
    "outputs_absolute_errors = np.abs(outputs - outputs_red)\n",
    "outputs_relative_errors = np.abs(outputs - outputs_red) / np.abs(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6904c1eb",
   "metadata": {},
   "source": [
    "The average absolute error (component-wise) on the training set is given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa09174",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(outputs_absolute_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eeb095",
   "metadata": {},
   "source": [
    "The average relative error is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832b9228",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(outputs_relative_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7cfb74",
   "metadata": {},
   "source": [
    "and the median of the speedups amounts to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97680102",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(outputs_speedups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed8c3d5-241c-4821-b340-35b741e616cb",
   "metadata": {},
   "source": [
    "## Neural networks for instationary problems\n",
    "\n",
    "To solve instationary problems using neural networks, we have extended the\n",
    "{class}`~pymor.reductors.neural_network.NeuralNetworkReductor` to the\n",
    "{class}`~pymor.reductors.neural_network.NeuralNetworkInstationaryReductor`, which treats time\n",
    "as an additional parameter (see {cite}`WHR19`). The resulting\n",
    "{class}`~pymor.models.neural_network.NeuralNetworkInstationaryModel` passes the input, together\n",
    "with the current time instance, through the neural network in each time step to obtain reduced\n",
    "coefficients. In the same fashion, there exists a\n",
    "{class}`~pymor.reductors.neural_network.NeuralNetworkInstationaryStatefreeOutputReductor` and the\n",
    "corresponding {class}`~pymor.models.neural_network.NeuralNetworkInstationaryStatefreeOutputModel`.\n",
    "\n",
    "A slightly different approach that is also implemented in pyMOR and uses a different type of\n",
    "neural network is described in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aab194-d612-4f4e-b1dc-1086d456a1f1",
   "metadata": {},
   "source": [
    "### Long short-term memory neural networks for instationary problems\n",
    "\n",
    "So-called *recurrent neural networks* are especially well-suited for capturing time-dependent\n",
    "dynamics. These types of neural networks can treat input sequences of variable length (in our case\n",
    "sequences with a variable number of time steps) and store internal states that are passed from one\n",
    "time step to the next. Therefore, these networks implement an internal memory that keeps\n",
    "information over time. Furthermore, for each element of the input sequence, the same neural\n",
    "network is applied.\n",
    "\n",
    "In the {class}`~pymor.models.neural_network.NeuralNetworkLSTMInstationaryModel` and the\n",
    "corresponding {class}`~pymor.reductors.neural_network.NeuralNetworkLSTMInstationaryReductor`,\n",
    "we make use of a specific type of recurrent neural network, namely a so-called\n",
    "*long short-term memory neural network (LSTM)*, first introduced in {cite}`HS97`, that tries to\n",
    "avoid problems like vanishing or exploding gradients that often occur during training of recurrent\n",
    "neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b016a49-9ffd-4e61-962d-000b40bdbb06",
   "metadata": {},
   "source": [
    "#### The architecture of an LSTM neural network\n",
    "In an LSTM neural network, multiple so-called LSTM cells are chained with each other such that the\n",
    "cell state {math}`c_k` and the hidden state {math}`h_k` of the {math}`k`-th LSTM cell serve as the\n",
    "input hidden states for the {math}`k+1`-th LSTM cell. Therefore, information from former time\n",
    "steps can be available later. Each LSTM cell takes an input {math}`\\mu(t_k)` and produces an\n",
    "output {math}`o(t_k)`. The following figure shows the general structure of an LSTM neural network\n",
    "that is also implemented in the same way in pyMOR:\n",
    "\n",
    "<center><img src=\"lstm.svg\" alt=\"Long short-term neural network\" width=\"100%\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd3d23a-9fcf-49a8-bacc-647878547baa",
   "metadata": {},
   "source": [
    "#### The LSTM cell\n",
    "The main building block of an LSTM network is the *LSTM cell*, which is denoted by {math}`\\Phi`,\n",
    "and sketched in the following figure:\n",
    "\n",
    "<img src=\"lstm_cell.svg\" alt=\"LSTM cell\" style=\"float: left;\" />\n",
    "\n",
    "Here, {math}`\\mu(t_k)` denotes the input of the network at the current time instance {math}`t_k`,\n",
    "while {math}`o(t_k)` denotes the output. The two hidden states for time instance `t_k` are given\n",
    "as the cell state {math}`c_k` and the hidden state {math}`h_k` that also serves as the output.\n",
    "Squares represent layers similar to those used in feedforward neural networks, where inside the\n",
    "square the applied activation function is mentioned, and circles denote element-wise\n",
    "operations like element-wise multiplication ({math}`\\times`), element-wise addition ({math}`+`) or\n",
    "element-wise application of the hyperbolic tangent function ({math}`\\tanh`). The filled black\n",
    "circle represents the concatenation of the inputs. Furthermore, {math}`\\sigma` is the sigmoid\n",
    "activation function ({math}`\\sigma(x)=\\frac{1}{1+\\exp(-x)}`), and {math}`\\tanh` is the hyperbolic\n",
    "tangent activation function ({math}`\\tanh(x)=\\frac{\\exp(x)-\\exp(-x)}{\\exp(x)+\\exp(-x)}`) used for\n",
    "the respective layers in the LSTM network. Finally, the layer {math}`P` denotes a projection layer\n",
    "that projects vectors of the internal size to the hidden and output size. Hence, internally, the\n",
    "LSTM can deal with larger quantities and finally projects them onto a space with a desired size.\n",
    "Altogether, a single LSTM cell takes two hidden states and an input of the form\n",
    "{math}`(c_{k-1},h_{k-1},\\mu(t_k))` and transforms them into new hidden states and an output state\n",
    "of the form {math}`(c_k,h_k,o(t_k))`.\n",
    "\n",
    "We will take a closer look at the individual components of an LSTM cell in the subsequent\n",
    "paragraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202cdb93-dad2-4526-b319-14cf4720c067",
   "metadata": {},
   "source": [
    "##### The forget gate\n",
    "\n",
    "<img src=\"lstm_cell_forget_gate.svg\" alt=\"Forget gate of an LSTM cell\" style=\"float: left;\" />\n",
    "\n",
    "As the name already suggests, the *forget gate* determines which part of the cell state\n",
    "{math}`c_{k-1}` the network forgets when moving to the next cell state {math}`c_k`. The main\n",
    "component of the forget gate is a neural network layer consisting of an affine-linear function\n",
    "with adjustable weights and biases followed by a sigmoid nonlinearity. By applying the sigmoid\n",
    "activation function, the output of the layer is scaled to lie between 0 and 1. The cell state\n",
    "{math}`c_{k-1}` from the previous cell is (point-wise) multiplied by the output of the layer in\n",
    "the forget gate. Hence, small values in the output of the layer correspond to parts of the cell\n",
    "state that are diminished, while values near 1 mean that the corresponding parts of the cell\n",
    "state remain intact. As input of the forget gate serves the pair {math}`(h_{k-1},\\mu(t_k))` and\n",
    "in the second step also the cell state {math}`c_{k-1}`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95917be7-f989-4adb-89e3-49cfdf47de0f",
   "metadata": {},
   "source": [
    "##### The input gate\n",
    "\n",
    "<img src=\"lstm_cell_input_gate.svg\" alt=\"Input gate of an LSTM cell\" style=\"float: left;\" />\n",
    "\n",
    "To further change the cell state, an LSTM cell contains a so-called *input gate*. This gate mainly\n",
    "consists of two layers, a sigmoid layer and an hyperbolic tangent layer, acting on the pair\n",
    "{math}`(h_{k-1},\\mu(t_k))`. As in the forget gate, the sigmoid layer determines which parts of the\n",
    "cell state to adjust. On the other hand, the hyperbolic tangent layer determines how to adjust the\n",
    "cell state. Using the hyperbolic tangent as activation function scales the output to be between -1\n",
    "and 1, and allows for small updates of the cell state. To finally compute the update, the outputs\n",
    "of the sigmoid and the hyperbolic tangent layer are multiplied entry-wise. Afterwards, the update\n",
    "is added to the cell state (after the cell state passed the forget gate). The new cell state is\n",
    "now prepared to be passed to the subsequent LSTM cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6ad171-6067-48e7-a55f-d7691828f98b",
   "metadata": {},
   "source": [
    "##### The output gate\n",
    "\n",
    "<img src=\"lstm_cell_output_gate.svg\" alt=\"Output gate of an LSTM cell\" style=\"float: left;\" />\n",
    "\n",
    "For computing the output {math}`o(t_k)` (and the new hidden state {math}`h_k`), the updated cell\n",
    "state {math}`c_k` is first of all entry-wise transformed using a hyperbolic tangent function such\n",
    "that the result again takes values between -1 and 1. Simultaneously, a neural network layer with a\n",
    "sigmoid activation function is applied to the concatenated pair {math}`(h_{k-1},\\mu(t_k))` of\n",
    "hidden state and input. Both results are multiplied entry-wise. This results in a filtered version\n",
    "of the (normalized) cell state. Finally, a projection layer is applied such that the result of the\n",
    "output gate has the desired size and can take arbitrary real values (before, due to the sigmoid and\n",
    "hyperbolic tangent activation functions, the outcome was restricted to the interval from -1 to 1).\n",
    "The projection layer applies a linear function without an activation (similar to the last layer of\n",
    "a usual feedforward neural network but without bias). Altogether, the *output gate* produces an\n",
    "output {math}`o(t_k)` that is returned and a new hidden state {math}`h_k` that can be passed\n",
    "(together with the updated cell state {math}`c_k`) to the next LSTM cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac873ed2-3f2f-4f73-b40e-1b68a8d48c82",
   "metadata": {},
   "source": [
    "#### LSTMs for model order reduction\n",
    "The idea of the approach implemented in pyMOR is the following: Instead of passing the current\n",
    "time instance as an additional input of the neural network, we use an LSTM that takes at each time\n",
    "instance {math}`t_k` the (potentially) time-dependent input {math}`\\mu(t_k)` as an input and uses\n",
    "the hidden states of the former time step. The output {math}`o(t_k)` of the LSTM (and therefore\n",
    "also the hidden state {math}`h_k`) at time {math}`t_k` are either approximations of the reduced\n",
    "basis coefficients (similar to the\n",
    "{class}`~pymor.models.neural_network.NeuralNetworkInstationaryModel`) or approximations of the\n",
    "output quantities (similar to the\n",
    "{class}`~pymor.models.neural_network.NeuralNetworkInstationaryModel`). For state approximations\n",
    "using a reduced basis, one can apply the \n",
    "{class}`~pymor.reductors.neural_network.NeuralNetworkLSTMInstationaryReductor` and use the\n",
    "corresponding\n",
    "{class}`~pymor.models.neural_network.NeuralNetworkLSTMInstationaryModel`.\n",
    "For a direct approximation of outputs using LSTMs, we provide the\n",
    "{class}`~pymor.models.neural_network.NeuralNetworkLSTMInstationaryStatefreeOutputModel` and the\n",
    "corresponding\n",
    "{class}`~pymor.reductors.neural_network.NeuralNetworkLSTMInstationaryStatefreeOutputReductor`."
   ]
  }
 ],
 "metadata": {
  "jupyter": {
   "jupytext": {
    "cell_metadata_filter": "-all",
    "formats": "ipynb,myst",
    "main_language": "python",
    "text_representation": {
     "extension": ".md",
     "format_name": "myst",
     "format_version": "1.3",
     "jupytext_version": "1.11.2"
    }
   }
  },
  "jupytext": {
   "text_representation": {
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
